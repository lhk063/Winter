{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600038142911",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers, Input\n",
    "from keras.models import Model, Input\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "from keras.layers import Conv2D, SeparableConv2D, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, BatchNormalization, Dropout\n",
    "from keras.layers import Add\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import keras.backend as K\n",
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.4.2'"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_radam import RAdam\n",
    "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
    "from tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "__all__ = ['RAdam']\n",
    "\n",
    "\n",
    "class RAdam(OptimizerV2):\n",
    "    \"\"\"RAdam optimizer.\n",
    "    According to the paper\n",
    "    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999,\n",
    "                 epsilon=1e-7,\n",
    "                 weight_decay=0.,\n",
    "                 amsgrad=False,\n",
    "                 total_steps=0,\n",
    "                 warmup_proportion=0.1,\n",
    "                 min_lr=0.,\n",
    "                 name='RAdam',\n",
    "                 **kwargs):\n",
    "        r\"\"\"Construct a new Adam optimizer.\n",
    "        Args:\n",
    "            learning_rate: A Tensor or a floating point value.    The learning rate.\n",
    "            beta_1: A float value or a constant float tensor. The exponential decay\n",
    "                rate for the 1st moment estimates.\n",
    "            beta_2: A float value or a constant float tensor. The exponential decay\n",
    "                rate for the 2nd moment estimates.\n",
    "            epsilon: A small constant for numerical stability. This epsilon is\n",
    "                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "            weight_decay: A floating point value. Weight decay for each param.\n",
    "            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n",
    "                the paper \"On the Convergence of Adam and beyond\".\n",
    "            total_steps: An integer. Total number of training steps.\n",
    "                Enable warmup by setting a positive value.\n",
    "            warmup_proportion: A floating point value. The proportion of increasing steps.\n",
    "            min_lr: A floating point value. Minimum learning rate after warmup.\n",
    "            name: Optional name for the operations created when applying gradients.\n",
    "                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n",
    "                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n",
    "                a callable that takes no arguments and returns the actual value to use.\n",
    "                This can be useful for changing these values across different\n",
    "                invocations of optimizer functions. @end_compatibility\n",
    "            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "                gradients by value, `decay` is included for backward compatibility to\n",
    "                allow time inverse decay of learning rate. `lr` is included for backward\n",
    "                compatibility, recommended to use `learning_rate` instead.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RAdam, self).__init__(name, **kwargs)\n",
    "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "        self._set_hyper('beta_1', beta_1)\n",
    "        self._set_hyper('beta_2', beta_2)\n",
    "        self._set_hyper('decay', self._initial_decay)\n",
    "        self._set_hyper('weight_decay', weight_decay)\n",
    "        self._set_hyper('total_steps', float(total_steps))\n",
    "        self._set_hyper('warmup_proportion', warmup_proportion)\n",
    "        self._set_hyper('min_lr', min_lr)\n",
    "        self.epsilon = epsilon or K.epsilon()\n",
    "        self.amsgrad = amsgrad\n",
    "        self._initial_weight_decay = weight_decay\n",
    "        self._initial_total_steps = total_steps\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v')\n",
    "        if self.amsgrad:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, 'vhat')\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[:len(params)]\n",
    "        super(RAdam, self).set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
    "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
    "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m_t = state_ops.assign(m,\n",
    "                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n",
    "                               use_locking=self._use_locking)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v_t = state_ops.assign(v,\n",
    "                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n",
    "                               use_locking=self._use_locking)\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat,\n",
    "                                      math_ops.maximum(vhat, v_t),\n",
    "                                      use_locking=self._use_locking)\n",
    "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                            sma_inf / sma_t)\n",
    "\n",
    "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
    "\n",
    "        if self._initial_weight_decay > 0.0:\n",
    "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
    "\n",
    "        var_update = state_ops.assign_sub(var,\n",
    "                                          lr_t * var_t,\n",
    "                                          use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper('total_steps', var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n",
    "            min_lr = self._get_hyper('min_lr', var_dtype)\n",
    "            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m = self.get_slot(var, 'm')\n",
    "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
    "        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v = self.get_slot(var, 'v')\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n",
    "        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat,\n",
    "                                      math_ops.maximum(vhat, v_t),\n",
    "                                      use_locking=self._use_locking)\n",
    "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
    "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
    "                            sma_inf / sma_t)\n",
    "\n",
    "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
    "\n",
    "        if self._initial_weight_decay > 0.0:\n",
    "            var_t += self._get_hyper('weight_decay', var_dtype) * var\n",
    "\n",
    "        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RAdam, self).get_config()\n",
    "        config.update({\n",
    "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "            'decay': self._serialize_hyperparameter('decay'),\n",
    "            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad,\n",
    "            'total_steps': self._serialize_hyperparameter('total_steps'),\n",
    "            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n",
    "            'min_lr': self._serialize_hyperparameter('min_lr'),\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv').set_index('id')\n",
    "test = pd.read_csv('data/test.csv').set_index('id')\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = train.copy()\n",
    "# copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, validation_data = train_test_split(copy, test_size = 0.2, shuffle = False, random_state = 1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "# validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = train_data['digit'].to_frame()\n",
    "letter = train_data['letter'].to_frame()\n",
    "\n",
    "y_val = validation_data['digit'].to_frame()\n",
    "letter_val = validation_data['letter'].to_frame()\n",
    "test_letter = test['letter'].to_frame()\n",
    "# ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = train_data.drop(['digit', 'letter'], axis = 1)\n",
    "x_val = validation_data.drop(['digit', 'letter'], axis = 1)\n",
    "test = test.drop(['letter'], axis = 1)\n",
    "# xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # digit  디코딩\n",
    "# decoding = np.argmax(onehot_ytrain, axis=1).reshape(-1,1)\n",
    "# decoding = pd.DataFrame(decoding, columns = ['digit'])\n",
    "# decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# onehot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "onehot_letter = encoder.fit_transform(letter)\n",
    "onehot_letter_val = encoder.fit_transform(letter_val)\n",
    "onehot_test_letter = encoder.fit_transform(test_letter)\n",
    "\n",
    "\n",
    "onehot_ytrain = tf.keras.utils.to_categorical(ytrain, 10)\n",
    "onehot_y_val = tf.keras.utils.to_categorical(y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.array(xtrain)\n",
    "ytrain = np.array(ytrain)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "test = np.array(test)\n",
    "\n",
    "onehot_letter = onehot_letter.toarray()\n",
    "onehot_letter_val = onehot_letter_val.toarray()\n",
    "onehot_test_letter = onehot_test_letter.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "xtrain.shape = (1638, 784)\nytrain.shape = (1638, 1)\nonehot_letter.shape = (1638, 26)\nx_val.shape = (410, 784)\ny_val.shape = (410, 1)\nonehot_letter_val.shape = (410, 26)\ntest.shape = (20480, 784)\nonehot_test_letter.shape = (20480, 26)\n"
    }
   ],
   "source": [
    "print('xtrain.shape = {}'.format(xtrain.shape))\n",
    "print('ytrain.shape = {}'.format(ytrain.shape))\n",
    "print('onehot_letter.shape = {}'.format(onehot_letter.shape))\n",
    "\n",
    "print('x_val.shape = {}'.format(x_val.shape))\n",
    "print('y_val.shape = {}'.format(y_val.shape))\n",
    "print('onehot_letter_val.shape = {}'.format(onehot_letter_val.shape))\n",
    "\n",
    "print('test.shape = {}'.format(test.shape))\n",
    "print('onehot_test_letter.shape = {}'.format(onehot_test_letter.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "xtrain = xtrain.reshape((1638, 28, 28, 1))\n",
    "onehot_letter = onehot_letter.reshape((1638, 1, 26))\n",
    "\n",
    "x_val = x_val.reshape((410, 28, 28, 1))\n",
    "onehot_letter_val = onehot_letter_val.reshape((410, 1, 26))\n",
    "\n",
    "test = test.reshape((20480, 28, 28, 1))\n",
    "onehot_test_letter = onehot_test_letter.reshape((20480, 1, 26))\n",
    "\n",
    "\n",
    "# float32\n",
    "xtrain = xtrain.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "onehot_letter = onehot_letter.astype('float32')\n",
    "\n",
    "onehot_ytrain = onehot_ytrain.astype('float32')\n",
    "onehot_y_val = onehot_y_val.astype('float32')\n",
    "onehot_letter_val = onehot_letter_val.astype('float32')\n",
    "\n",
    "test = test.astype('float32')\n",
    "onehot_test_letter = onehot_test_letter.astype('float32') \n",
    "\n",
    "\n",
    "# /255\n",
    "xtrain /= 255.0\n",
    "x_val /= 255.0\n",
    "test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "xtrain.shape = (1638, 28, 28, 1)\nytrain.shape = (1638, 1)\nonehot_letter.shape = (1638, 1, 26)\nx_val.shape = (410, 28, 28, 1)\ny_val.shape = (410, 1)\nonehot_letter_val.shape = (410, 1, 26)\ntest.shape = (20480, 28, 28, 1)\nonehot_test_letter.shape = (20480, 1, 26)\n"
    }
   ],
   "source": [
    "print('xtrain.shape = {}'.format(xtrain.shape))\n",
    "print('ytrain.shape = {}'.format(ytrain.shape))\n",
    "print('onehot_letter.shape = {}'.format(onehot_letter.shape))\n",
    "\n",
    "print('x_val.shape = {}'.format(x_val.shape))\n",
    "print('y_val.shape = {}'.format(y_val.shape))\n",
    "print('onehot_letter_val.shape = {}'.format(onehot_letter_val.shape))\n",
    "\n",
    "print('test.shape = {}'.format(test.shape))\n",
    "print('onehot_test_letter.shape = {}'.format(onehot_test_letter.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding['digit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZER = tf.keras.optimizers.SGD(lr=0.01)\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    \n",
    "    width_shift_range = 5,\n",
    "    height_shift_range = 5,\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.05\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "# train_datagen.fit(xtrain)\n",
    "# validation_datagen.fit(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = Input(shape = (28, 28, 1), dtype = 'float32', name = 'image')\n",
    "letter_input = Input(shape = (1, 26), dtype = 'float32', name = 'letter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ion_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 14, 14, 128)  8320        batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nadd (Add)                       (None, 14, 14, 128)  0           max_pooling2d[0][0]              \n                                                                 conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nseparable_conv2d_2 (SeparableCo (None, 14, 14, 256)  34176       add[0][0]                        \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 14, 14, 256)  1024        separable_conv2d_2[0][0]         \n__________________________________________________________________________________________________\nseparable_conv2d_3 (SeparableCo (None, 14, 14, 256)  68096       batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 14, 14, 256)  1024        separable_conv2d_3[0][0]         \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 256)    0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 7, 7, 256)    33024       add[0][0]                        \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 7, 7, 256)    0           max_pooling2d_1[0][0]            \n                                                                 conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nseparable_conv2d_4 (SeparableCo (None, 7, 7, 728)    189400      add_1[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 7, 7, 728)    2912        separable_conv2d_4[0][0]         \n__________________________________________________________________________________________________\nseparable_conv2d_5 (SeparableCo (None, 7, 7, 728)    537264      batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 7, 7, 728)    2912        separable_conv2d_5[0][0]         \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 728)    0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 4, 4, 728)    187096      add_1[0][0]                      \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 4, 4, 728)    0           max_pooling2d_2[0][0]            \n                                                                 conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nseparable_conv2d_6 (SeparableCo (None, 4, 4, 728)    537264      add_2[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 4, 4, 728)    2912        separable_conv2d_6[0][0]         \n__________________________________________________________________________________________________\nseparable_conv2d_7 (SeparableCo (None, 4, 4, 728)    537264      batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 4, 4, 728)    2912        separable_conv2d_7[0][0]         \n__________________________________________________________________________________________________\nseparable_conv2d_8 (SeparableCo (None, 4, 4, 728)    537264      batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_8[0][0]         \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 4, 4, 728)    0           batch_normalization_10[0][0]     \n                                                                 add_2[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_9 (SeparableCo (None, 4, 4, 728)    537264      add_3[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_9[0][0]         \n__________________________________________________________________________________________________\nseparable_conv2d_10 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_10[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_11 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_11[0][0]        \n__________________________________________________________________________________________________\nadd_4 (Add)                     (None, 4, 4, 728)    0           batch_normalization_13[0][0]     \n                                                                 add_3[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_12 (SeparableC (None, 4, 4, 728)    537264      add_4[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_12[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_13 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_13[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_14 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_14[0][0]        \n__________________________________________________________________________________________________\nadd_5 (Add)                     (None, 4, 4, 728)    0           batch_normalization_16[0][0]     \n                                                                 add_4[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_15 (SeparableC (None, 4, 4, 728)    537264      add_5[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_15[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_16 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_16[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_17 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_17[0][0]        \n__________________________________________________________________________________________________\nadd_6 (Add)                     (None, 4, 4, 728)    0           batch_normalization_19[0][0]     \n                                                                 add_5[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_18 (SeparableC (None, 4, 4, 728)    537264      add_6[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_18[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_19 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_19[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_20 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_20[0][0]        \n__________________________________________________________________________________________________\nadd_7 (Add)                     (None, 4, 4, 728)    0           batch_normalization_22[0][0]     \n                                                                 add_6[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_21 (SeparableC (None, 4, 4, 728)    537264      add_7[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_21[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_22 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_22[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_23 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_24[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_25 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_23[0][0]        \n__________________________________________________________________________________________________\nadd_8 (Add)                     (None, 4, 4, 728)    0           batch_normalization_25[0][0]     \n                                                                 add_7[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_24 (SeparableC (None, 4, 4, 728)    537264      add_8[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_26 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_24[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_25 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_26[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_27 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_25[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_26 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_27[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_28 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_26[0][0]        \n__________________________________________________________________________________________________\nadd_9 (Add)                     (None, 4, 4, 728)    0           batch_normalization_28[0][0]     \n                                                                 add_8[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_27 (SeparableC (None, 4, 4, 728)    537264      add_9[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_29 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_27[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_28 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_29[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_30 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_28[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_29 (SeparableC (None, 4, 4, 728)    537264      batch_normalization_30[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_29[0][0]        \n__________________________________________________________________________________________________\nadd_10 (Add)                    (None, 4, 4, 728)    0           batch_normalization_31[0][0]     \n                                                                 add_9[0][0]                      \n__________________________________________________________________________________________________\nseparable_conv2d_30 (SeparableC (None, 4, 4, 728)    537264      add_10[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_30[0][0]        \n__________________________________________________________________________________________________\nseparable_conv2d_31 (SeparableC (None, 4, 4, 1024)   753048      batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 4, 4, 1024)   4096        separable_conv2d_31[0][0]        \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 2, 2, 1024)   0           batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 2, 2, 1024)   746496      add_10[0][0]                     \n__________________________________________________________________________________________________\nadd_11 (Add)                    (None, 2, 2, 1024)   0           max_pooling2d_3[0][0]            \n                                                                 conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nseparable_conv2d_32 (SeparableC (None, 2, 2, 1536)   1583616     add_11[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_34 (BatchNo (None, 2, 2, 1536)   6144        separable_conv2d_32[0][0]        \n__________________________________________________________________________________________________\nletter (InputLayer)             [(None, 1, 26)]      0                                            \n__________________________________________________________________________________________________\nseparable_conv2d_33 (SeparableC (None, 2, 2, 2048)   3161600     batch_normalization_34[0][0]     \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 1, 128)       79360       letter[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_35 (BatchNo (None, 2, 2, 2048)   8192        separable_conv2d_33[0][0]        \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 1, 256)       394240      lstm[0][0]                       \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 2048)         0           batch_normalization_35[0][0]     \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 256)          525312      lstm_1[0][0]                     \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 2304)         0           global_average_pooling2d[0][0]   \n                                                                 lstm_2[0][0]                     \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 512)          1180160     concatenate[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_36 (BatchNo (None, 512)          2048        dense[0][0]                      \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 512)          0           batch_normalization_36[0][0]     \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 10)           5130        dropout[0][0]                    \n==================================================================================================\nTotal params: 23,065,874\nTrainable params: 23,014,594\nNon-trainable params: 51,280\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "## Entry flow\n",
    "\n",
    "\n",
    "cnn = layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same')(image_input)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "\n",
    "previous_block = cnn\n",
    "\n",
    "for fliters in [128, 256, 728]:\n",
    "    \n",
    "    residual = layers.Conv2D(fliters, (1, 1), activation = None, padding = 'same', strides = 2)(previous_block)\n",
    "   \n",
    "    cnn = layers.SeparableConv2D(fliters, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = layers.SeparableConv2D(fliters, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    cnn = MaxPooling2D((3, 3), padding='same', strides=2)(cnn)\n",
    "    \n",
    "\n",
    "    cnn = tf.keras.layers.Add()([cnn, residual])\n",
    "    previous_block = cnn\n",
    "\n",
    "\n",
    "## Middle flow\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    cnn = layers.SeparableConv2D(728, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "\n",
    "    cnn = layers.SeparableConv2D(728, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "\n",
    "    cnn = layers.SeparableConv2D(728, (3, 3), activation = None, padding = 'same')(cnn)\n",
    "    cnn = layers.BatchNormalization()(cnn)\n",
    "    \n",
    "    cnn = tf.keras.layers.Add()([cnn, previous_block])\n",
    "    previous_block = cnn\n",
    "\n",
    "\n",
    "## Exit flow\n",
    "\n",
    "\n",
    "previous_block = cnn\n",
    "    \n",
    "cnn = layers.SeparableConv2D(728, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "\n",
    "cnn = layers.SeparableConv2D(1024, (3, 3), activation=None, padding = 'same')(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "\n",
    "cnn = MaxPooling2D((3, 3), padding='same', strides=2)(cnn) # (19, 19, 1024) -> (10, 10, 1024)\n",
    "\n",
    "residual = layers.Conv2D(1024, (1, 1), strides=2, activation=None, padding = 'same')(previous_block) # (19, 19, 728) -> (10, 10, 1024)\n",
    "cnn = tf.keras.layers.Add()([cnn, residual])\n",
    "\n",
    "cnn = layers.SeparableConv2D(1536, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "cnn = layers.SeparableConv2D(2048, (3, 3), activation = 'relu', padding = 'same')(cnn)\n",
    "cnn = layers.BatchNormalization()(cnn)\n",
    "\n",
    "cnn = GlobalAveragePooling2D()(cnn)\n",
    "\n",
    "# letter LSTM\n",
    "dnn = layers.LSTM(128, activation = 'relu', return_sequences = True)(letter_input)\n",
    "dnn = layers.LSTM(256, activation = 'relu', return_sequences = True)(dnn)\n",
    "dnn = layers.LSTM(256, activation = 'relu')(dnn)\n",
    "\n",
    "concatenated = layers.concatenate([cnn, dnn])\n",
    "\n",
    "\n",
    "result = layers.Dense(512, activation = 'relu')(concatenated)\n",
    "result = layers.BatchNormalization()(result)\n",
    "result = layers.Dropout(0.5)(result)\n",
    "result = layers.Dense(10, activation = 'softmax')(result)\n",
    "\n",
    "model = Model([image_input, letter_input], result)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(RAdam(learning_rate = 0.001),\n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def Create_model():\n",
    "\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         keras.layers.ZeroPadding2D((1, 1), input_shape = (28, 28, 1)),\n",
    "#         keras.layers.Conv2D(128, (5, 5), activation = 'relu', padding = 'same'),\n",
    "#         # keras.layers.BatchNormalization(),\n",
    "#         # keras.layers.ZeroPadding2D((1, 1)),\n",
    "#         keras.layers.Conv2D(128, (3, 3), activation = 'relu', padding = 'same'),\n",
    "#         keras.layers.MaxPooling2D(2, 2),\n",
    "#         # keras.layers.BatchNormalization(),\n",
    "#         # keras.layers.ZeroPadding2D((1, 1)),\n",
    "#         keras.layers.Conv2D(256, (3, 3), activation = 'relu', padding = 'same'),\n",
    "#         # keras.layers.BatchNormalization(),\n",
    "#         # keras.layers.ZeroPadding2D((1, 1)),\n",
    "#         keras.layers.Conv2D(256, (3, 3), activation = 'relu', padding = 'same'),\n",
    "#         keras.layers.MaxPooling2D(2, 2),\n",
    "#         keras.layers.Dropout(0.2),\n",
    "#         keras.layers.Flatten(),\n",
    "#         # keras.layers.BatchNormalization(),\n",
    "#         keras.layers.Dense(256, activation = 'relu'),\n",
    "#         keras.layers.Dropout(0.5),\n",
    "#         # keras.layers.BatchNormalization(),\n",
    "#         keras.layers.Dense(10, activation = 'softmax')\n",
    "#     ])\n",
    "\n",
    "#     model.compile(RAdam(),\n",
    "#                   loss = 'categorical_crossentropy',\n",
    "#                   metrics = [\"accuracy\"])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model = Create_model()\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 10)\n",
    "ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', save_weights_only = True, save_best_only = True)\n",
    "ReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 5, factor = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/150\n103/102 [==============================] - 75s 733ms/step - loss: 3.3110 - accuracy: 0.1545 - val_loss: 2.3044 - val_accuracy: 0.1073\nEpoch 2/150\n103/102 [==============================] - 79s 767ms/step - loss: 3.1196 - accuracy: 0.1734 - val_loss: 2.3409 - val_accuracy: 0.1049\nEpoch 3/150\n103/102 [==============================] - 79s 766ms/step - loss: 2.8129 - accuracy: 0.2063 - val_loss: 2.3810 - val_accuracy: 0.1073\nEpoch 4/150\n103/102 [==============================] - 75s 727ms/step - loss: 2.5542 - accuracy: 0.2460 - val_loss: 2.3675 - val_accuracy: 0.1049\nEpoch 5/150\n103/102 [==============================] - 75s 724ms/step - loss: 2.4848 - accuracy: 0.2662 - val_loss: 2.6153 - val_accuracy: 0.0683\nEpoch 6/150\n103/102 [==============================] - 78s 758ms/step - loss: 2.2358 - accuracy: 0.3095 - val_loss: 2.1244 - val_accuracy: 0.2000\nEpoch 7/150\n 21/102 [=====>........................] - ETA: 53s - loss: 2.2586 - accuracy: 0.3067"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-76bb2383ec9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_letter_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_y_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2020\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m           \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m           \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
    "# model.fit_generator(\n",
    "#     datagen.flow(xtrain, ytrain1, batch_size = 32),\n",
    "#     epochs = 150,\n",
    "#     verbose = 1,\n",
    "#     callbacks = [tensorboard, checkpoint]\n",
    "# )\n",
    "model.fit(train_datagen.flow([xtrain, onehot_letter], onehot_ytrain, seed = 2020, batch_size = 16),  \n",
    "          epochs = 150,\n",
    "          steps_per_epoch = len(xtrain) / 16,\n",
    "          verbose = 1,\n",
    "          validation_data = validation_datagen.flow([x_val, onehot_letter_val], onehot_y_val, seed = 2020, batch_size = 16),\n",
    "          validation_steps = len(x_val) / 16,\n",
    "          callbacks = [ModelCheckpoint]\n",
    ")\n",
    "\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorboard Link](http://localhost:6006)  \n",
    "71/71 [==============================] - 13s 181ms/step - loss: 0.0737 - accuracy: 0.9756 - val_loss: 0.5013 - val_accuracy: 0.8244  85%\n",
    "\n",
    "Epoch 11/150  \n",
    "58/58 [==============================] - 18s 316ms/step - loss: 0.2306 - accuracy: 0.9143 - val_loss: 0.4259 - val_accuracy: 0.8585 - lr: 0.0010 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1728c1373a36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_test_letter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# digit  디코딩\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'digit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([test, onehot_test_letter], batch_size=32)\n",
    "# digit  디코딩\n",
    "decoding = np.argmax(y_pred, axis=1).reshape(-1,1)\n",
    "decoding = pd.DataFrame(decoding, columns = ['digit'])\n",
    "decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'decoding' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-184d77bd9d1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'digit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'decoding' is not defined"
     ]
    }
   ],
   "source": [
    "sorted(decoding['digit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3c5e6d198bee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_test_letter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# digit  디코딩\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'digit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_model.h5')\n",
    "y_pred = model.predict([test, onehot_test_letter], batch_size=26)\n",
    "# digit  디코딩\n",
    "decoding = np.argmax(y_pred, axis=1).reshape(-1,1)\n",
    "decoding = pd.DataFrame(decoding, columns = ['digit'])\n",
    "decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'decoding' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a6cf18086f82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'digit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sub.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoding' is not defined"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('data/submission.csv')\n",
    "sub = sub.drop(['digit'], axis = 1)\n",
    "sub = pd.concat([sub, decoding], axis = 1)\n",
    "sub.to_csv('sub.csv', index = False, encoding = 'utf-8')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}